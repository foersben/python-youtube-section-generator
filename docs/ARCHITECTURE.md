# Architecture & Technical Design

This document explains the technical architecture, design decisions, and implementation details of the YouTube Transcript Section Generator.

## ðŸ—ï¸ System Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Interface â”‚    â”‚   CLI Tool      â”‚    â”‚   API Service   â”‚
â”‚   (Flask)       â”‚    â”‚   (main.py)     â”‚    â”‚   (Future)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
                    â”‚  SectionGenerationService â”‚                    â”‚
                    â”‚  (Business Logic)        â”‚                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                                 â”‚                                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
          â”‚                                             â”‚          â”‚
          â”‚            Core Processing Pipeline         â”‚          â”‚
          â”‚                                             â”‚          â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                 â”‚                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
                    â”‚                         â”‚                    â”‚
                    â”‚    Translation Service  â”‚                    â”‚
                    â”‚    (DeepL Adapter)      â”‚                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                                 â”‚                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
                    â”‚                         â”‚                    â”‚
                    â”‚      RAG System         â”‚                    â”‚
                    â”‚   (ChromaDB + LLM)      â”‚                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                                 â”‚                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
                    â”‚                         â”‚                    â”‚
                    â”‚   Local LLM Service     â”‚                    â”‚
                    â”‚   (Phi-3-mini)          â”‚                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                                                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚                                             â”‚ â”‚
                    â”‚         External Dependencies               â”‚ â”‚
                    â”‚   â€¢ YouTube Transcript API                  â”‚ â”‚
                    â”‚   â€¢ DeepL Translation API                   â”‚ â”‚
                    â”‚   â€¢ HuggingFace Models                      â”‚ â”‚
                    â”‚                                             â”‚ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚                                             â”‚ â”‚
                    â”‚         Data Flow                           â”‚ â”‚
                    â”‚   Video URL â†’ Transcript â†’ Sections â†’ Outputâ”‚ â”‚
                    â”‚                                             â”‚ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
```

## ðŸ“ Package Structure

### Core Architecture

```
src/
â”œâ”€â”€ core/                          # Business logic & core functionality
â”‚   â”œâ”€â”€ __init__.py               # Core exports
â”‚   â”œâ”€â”€ formatting.py             # Output formatting utilities
â”‚   â”œâ”€â”€ config/                   # Configuration management
â”‚   â”œâ”€â”€ models/                   # Data models & DTOs
â”‚   â”œâ”€â”€ transcript/               # Transcript extraction & processing
â”‚   â”œâ”€â”€ services/                 # Business services (Facade pattern)
â”‚   â”œâ”€â”€ adapters/                 # External service adapters
â”‚   â”œâ”€â”€ llm/                      # LLM provider abstractions
â”‚   â”œâ”€â”€ embeddings/               # Embedding provider abstractions
â”‚   â””â”€â”€ retrieval/                # RAG system implementation
â”œâ”€â”€ utils/                         # Shared utilities
â”‚   â”œâ”€â”€ __init__.py               # Utility exports
â”‚   â”œâ”€â”€ logging_config.py         # Centralized logging system
â”‚   â”œâ”€â”€ file_io.py                # File operations
â”‚   â””â”€â”€ json_utils.py             # JSON manipulation
â”œâ”€â”€ templates/                     # Flask HTML templates
â”œâ”€â”€ static/                        # Static web assets
â”œâ”€â”€ main.py                        # CLI entry point
â””â”€â”€ web_app.py                     # Flask application
```

### Design Patterns Applied

#### Service Layer Pattern
- **SectionGenerationService**: Facade for complex section generation logic
- **TranslationService**: Unified interface for translation operations
- **RAGSystem**: High-level RAG operations abstraction

#### Adapter Pattern
- **DeepLAdapter**: Wraps DeepL API with consistent interface
- **LocalLLMAdapter**: Wraps llama.cpp with unified LLM interface
- **EmbeddingsAdapter**: Unified interface for different embedding providers

#### Factory Pattern
- **LLMFactory**: Creates appropriate LLM provider based on configuration
- **EmbeddingsFactory**: Creates embedding provider with device selection

#### Strategy Pattern
- **Translation Strategies**: DeepL vs local LLM vs none
- **Section Generation Strategies**: RAG vs direct LLM vs heuristics

## ðŸ”„ Processing Pipeline Details

### Phase 1: Input Processing & Validation

#### Video URL Processing
```python
# Extract video ID from various URL formats
def extract_video_id(url: str) -> str:
    pattern = r"(?:v=|/)([0-9A-Za-z_-]{11})"
    match = re.search(pattern, url)
    return match.group(1) if match else url
```

#### Transcript Extraction
```python
# YouTube Transcript API with fallbacks
transcript_list = YouTubeTranscriptApi.get_transcript(
    video_id,
    languages=['de', 'en']  # German first, English fallback
)
```

#### Data Normalization
```python
# Convert to internal format
segments = [
    {
        "start": float(seg["start"]),
        "text": seg["text"].strip(),
        "duration": float(seg.get("duration", 0.0))
    }
    for seg in transcript_list
]
```

### Phase 2: Translation Pipeline

#### Language Detection
```python
from langdetect import detect
sample_text = " ".join(seg["text"] for seg in transcript[:50])
source_lang = detect(sample_text)  # Returns "de", "en", etc.
```

#### Intelligent Batching Algorithm
```python
def create_batches(segments: list, max_chars: int = 4500) -> list[str]:
    batches = []
    current_batch = []
    current_length = 0

    for segment in segments:
        segment_text = f"[SEG_{len(current_batch)}]{segment['text']}"

        if current_length + len(segment_text) > max_chars and current_batch:
            # Finalize current batch
            batch_text = "\n".join(current_batch)
            batches.append(batch_text)
            current_batch = [segment_text]
            current_length = len(segment_text)
        else:
            current_batch.append(segment_text)
            current_length += len(segment_text)

    # Add final batch
    if current_batch:
        batches.append("\n".join(current_batch))

    return batches
```

#### DeepL API Integration
```python
# Single batch translation
response = requests.post(
    "https://api-free.deepl.com/v2/translate",
    data={
        "text": batch_text,
        "source_lang": source_lang.upper(),  # "DE"
        "target_lang": "EN-US",              # Consistent English
        "auth_key": DEEPL_API_KEY
    }
)

# Parse response and reconstruct segments
translated_segments = reconstruct_from_markers(response.text)
```

### Phase 3: RAG System Architecture

#### Text Chunking Strategy
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,           # Optimal for semantic coherence
    chunk_overlap=200,         # 20% overlap for context preservation
    separators=["\n\n", "\n", ". ", " ", ""],  # Smart splitting
    length_function=len
)

chunks = splitter.create_documents(
    texts=[full_transcript_text],
    metadatas=[{"video_id": video_id}]
)
```

#### Vector Store Configuration
```python
from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,      # SentenceTransformer wrapper
    persist_directory=f".chromadb/{video_hash}/",
    collection_name=f"transcript_{video_hash}"
)
```

#### Semantic Retrieval
```python
# Context-aware retrieval
docs = vectorstore.similarity_search(
    query=f"Context around {timestamp} seconds",
    k=min(10, retrieval_k),    # Adaptive retrieval count
    fetch_k=20                  # Internal candidates
)
```

### Phase 4: Multi-Stage LLM Refinement

#### Stage 1: Keyword Extraction
```python
stage1_prompt = f"""
Extract 3-5 key topics from this text:
{snippet[:200]}

Topics:
"""

keywords_result = llm.invoke(stage1_prompt)
keywords = str(keywords_result).strip()
```

#### Stage 2: Title Synthesis
```python
stage2_prompt = f"""
Create a 2-4 word title from these topics: {keywords}
Use nouns only. No verbs.
Title:
"""

title_result = llm.invoke(stage2_prompt)
raw_title = str(title_result).strip()
```

#### Stage 3: Post-Processing Polish
```python
def polish_title(title: str) -> str:
    # Remove artifacts
    artifacts = ["assistant", "response", "title", "topics"]
    for artifact in artifacts:
        title = re.sub(rf'\b{artifact}\b', '', title, flags=re.IGNORECASE)

    # Clean punctuation and capitalize
    title = title.strip('"`\'.,;:!?-â€“â€”')
    title = title[0].upper() + title[1:] if title else "Section"

    return title
```

### Phase 5: Hierarchical Section Generation

#### Main Section Distribution
```python
def calculate_main_anchors(total_duration: float, count: int) -> list[float]:
    """Distribute main sections evenly across video duration."""
    interval = total_duration / (count + 1)
    return [interval * (i + 1) for i in range(count)]
```

#### Subsection Distribution Algorithm
```python
def distribute_subsections(
    main_start: float,
    next_main_start: float,
    subsections_per_main: int
) -> list[float]:
    """Distribute subsections evenly within main section window."""

    window_start = main_start
    window_end = max(window_start + 1.0, next_main_start)
    window_length = window_end - window_start

    # Linear interpolation
    positions = []
    for i in range(subsections_per_main):
        fraction = (i + 1) / (subsections_per_main + 1)
        position = window_start + fraction * window_length
        positions.append(min(position, total_duration))

    return positions
```

### Phase 6: Output Formatting & Validation

#### YouTube-Compatible Timestamps
```python
def format_timestamp(seconds: float) -> str:
    """Format timestamp in YouTube-clickable format."""
    total_seconds = int(seconds)
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    seconds = total_seconds % 60

    if hours > 0:
        return f"{hours}:{minutes:02d}:{seconds:02d}"
    else:
        return f"{minutes:02d}:{seconds:02d}"
```

#### Hierarchical Display Formatting
```python
def format_sections_hierarchical(sections: list[dict]) -> str:
    """Format sections with proper hierarchy."""
    output = []
    main_counter = 0
    sub_counters = {}

    for section in sections:
        start = section["start"]
        title = section["title"]
        level = section.get("level", 0)
        timestamp = format_timestamp(start)

        if level == 0:
            # Main section
            main_counter += 1
            sub_counters[main_counter] = 0
            output.append(f"{main_counter}. {timestamp} {title}")
        else:
            # Subsection
            current_main = max(sub_counters.keys())
            sub_counters[current_main] += 1
            sub_letter = chr(ord('a') + (sub_counters[current_main] - 1))
            indent = "   "
            output.append(f"{indent}{sub_letter}. {timestamp} {title}")

    return "\n".join(output)
```

## ðŸ”§ Configuration Management

### Environment Variable Schema
```python
# Translation Configuration
DEEPL_API_KEY: str | None          # DeepL API key
USE_TRANSLATION: bool = True       # Enable translation pipeline
TRANSLATE_TO: str = "en"           # Target language

# Local LLM Configuration
USE_LOCAL_LLM: bool = True         # Use local Phi-3-mini
LOCAL_MODEL_PATH: str              # Path to GGUF model
USE_LLM_TITLES: bool = True        # Enable multi-stage refinement

# RAG Configuration
USE_RAG: str = "auto"              # auto, always, never
RAG_HIERARCHICAL: bool = True      # Hierarchical sections
LLM_TEMPERATURE: float = 0.05      # Deterministic output

# CLI Configuration
DEFAULT_VIDEO_ID: str = "kXhCEyix180"
OUTPUT_DIR: str = "."
MIN_SECTIONS: int = 10
MAX_SECTIONS: int = 15

# Logging Configuration
LOG_LEVEL: str = "INFO"
LOG_TO_FILE: bool = False
LOG_FILE: str = "logs/app.log"
```

### Configuration Validation
```python
def validate_config() -> dict[str, Any]:
    """Validate and normalize configuration."""
    config = {}

    # Translation settings
    config["use_translation"] = os.getenv("USE_TRANSLATION", "true").lower() == "true"
    config["deepl_key"] = os.getenv("DEEPL_API_KEY")

    if config["use_translation"] and not config["deepl_key"]:
        logger.warning("Translation enabled but no DEEPL_API_KEY found")

    # LLM settings
    config["use_local_llm"] = os.getenv("USE_LOCAL_LLM", "true").lower() == "true"
    config["model_path"] = os.getenv("LOCAL_MODEL_PATH", "models/Phi-3-mini-4k-instruct-q4.gguf")

    # Validate model exists
    if config["use_local_llm"] and not Path(config["model_path"]).exists():
        raise FileNotFoundError(f"Model not found: {config['model_path']}")

    return config
```

## ðŸš€ Performance Optimizations

### Translation Batching
- **Problem**: 1586 individual API calls for 1-hour video
- **Solution**: 32 batched calls with segment markers
- **Benefit**: 50x fewer API calls, 10x faster processing
- **Quality**: Context preserved within batches

### Multi-Stage LLM Pipeline
- **Problem**: Small 4B parameter model struggles with complex prompts
- **Solution**: Break reasoning into simple stages
- **Benefit**: 6x higher success rate, coherent titles
- **Fallback**: Heuristic cleaning when LLM fails

### CPU-Only Optimization
- **Problem**: GPU dependencies bloat installation
- **Solution**: Pure CPU inference with optimized packages
- **Benefit**: Smaller footprint, easier deployment
- **Performance**: Sufficient for production use

### Resource Management
- **Memory**: Proper cleanup of models and vector stores
- **Connections**: Explicit closing of API connections
- **Files**: Temporary file cleanup and rotation
- **Logging**: Efficient log rotation and filtering

## ðŸ§ª Quality Assurance

### Automated Testing Strategy
```python
# Unit tests for core functions
def test_extract_video_id():
    assert extract_video_id("https://youtu.be/dQw4w9WgXcQ") == "dQw4w9WgXcQ"

def test_format_timestamp():
    assert format_timestamp(3661) == "1:01:01"

# Integration tests for pipelines
def test_translation_pipeline():
    german_text = "Hallo Welt"
    english_text = translate_to_english(german_text)
    german_back = translate_to_german(english_text)
    assert german_back == german_text  # Round-trip accuracy

# Performance benchmarks
def benchmark_processing():
    # Measure end-to-end processing time
    # Validate output quality metrics
    # Check resource usage
```

### Error Handling Strategy
```python
class PipelineError(Exception):
    """Base exception for pipeline errors."""
    pass

class TranslationError(PipelineError):
    """Translation service failures."""
    pass

class LLMError(PipelineError):
    """LLM generation failures."""
    pass

def handle_pipeline_error(error: Exception, context: str) -> None:
    """Centralized error handling with context."""
    logger.error(f"Pipeline error in {context}: {error}")

    # Attempt recovery strategies
    if isinstance(error, TranslationError):
        logger.info("Falling back to no translation")
        # Continue without translation
    elif isinstance(error, LLMError):
        logger.info("Falling back to heuristic titles")
        # Use fallback title generation

    # Always attempt cleanup
    cleanup_resources()
```

## ðŸ”’ Security Considerations

### Input Validation
- **URL Sanitization**: Regex-based video ID extraction
- **Content Filtering**: No execution of user-provided code
- **Path Safety**: Restricted file operations to project directory

### API Security
- **Key Management**: Environment variables for API keys
- **Rate Limiting**: Respectful API usage with built-in delays
- **Error Handling**: No sensitive information in error messages

### Data Privacy
- **Local Processing**: All transcript data processed locally
- **No Persistence**: Temporary files cleaned up automatically
- **Minimal Logging**: No transcript content in logs

## ðŸ“Š Monitoring & Observability

### Logging Architecture
```python
# Centralized logging configuration
logger = setup_logging(
    level=os.getenv("LOG_LEVEL", "INFO"),
    log_file=os.getenv("LOG_FILE") if os.getenv("LOG_TO_FILE") == "true" else None,
    colored=sys.stdout.isatty()
)

# Structured logging with context
logger.info("Processing video", extra={
    "video_id": video_id,
    "duration": duration,
    "language": detected_lang
})
```

### Performance Metrics
```python
class PerformanceTracker:
    def __init__(self):
        self.metrics = {}

    def start_operation(self, name: str):
        self.metrics[name] = {"start": time.time()}

    def end_operation(self, name: str):
        if name in self.metrics:
            duration = time.time() - self.metrics[name]["start"]
            logger.info(f"Operation {name} completed in {duration:.2f}s")
            self.metrics[name]["duration"] = duration
```

### Health Checks
```python
def system_health_check() -> dict[str, bool]:
    """Check system components health."""
    health = {}

    # Check model availability
    health["model"] = Path("models/Phi-3-mini-4k-instruct-q4.gguf").exists()

    # Check API connectivity
    try:
        # Test DeepL API (if configured)
        health["translation"] = test_deepl_connection()
    except:
        health["translation"] = False

    # Check disk space
    stat = os.statvfs(".")
    free_gb = (stat.f_bavail * stat.f_frsize) / (1024**3)
    health["disk_space"] = free_gb > 2.0  # Need 2GB for models

    return health
```

## ðŸš€ Deployment Considerations

### Docker Configuration
```dockerfile
FROM python:3.11-slim

# Install system dependencies for llama.cpp
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY pyproject.toml poetry.lock ./
RUN pip install poetry && poetry install --no-dev

# CPU-only PyTorch
RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Copy application
COPY src/ ./src/
COPY models/ ./models/

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s \
    CMD python -c "from src.core.services.section_generation import SectionGenerationService; print('OK')"

EXPOSE 5000
CMD ["python", "src/web_app.py"]
```

### Production Environment Variables
```bash
# Production settings
LOG_LEVEL=WARNING
LOG_TO_FILE=true
LOG_FILE=/var/log/youtube-transcript.log

# Performance tuning
USE_RAG=always  # Always use RAG for consistency
LLM_TEMPERATURE=0.05  # Deterministic output

# Resource limits
MAX_VIDEO_LENGTH=7200  # 2 hours maximum
BATCH_SIZE=4500  # Translation batch size
```

This architecture represents a production-ready system optimized for reliability, performance, and maintainability.
