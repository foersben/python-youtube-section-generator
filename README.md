# üé¨ YouTube Transcript Section Generator

**Automatically generate intelligent, timestamped chapter sections from YouTube videos using AI.**

Extract transcripts, translate to English for better analysis, generate meaningful sections with local LLM refinement, and get clean German (or original language) titles back - all optimized for speed and quality.

## ‚ú® Key Features

- üéØ **Smart Section Generation**: Multi-stage LLM pipeline for high-quality titles
- üåç **Translation Pipeline**: Batched DE‚ÜíEN‚ÜíDE translation (50x faster than per-segment)
- üß† **RAG System**: ChromaDB + semantic search for long videos (1h+)
- ü§ñ **Local LLM**: CPU-optimized Phi-3-mini (GGUF) with multi-stage refinement
- ‚ö° **Blazing Fast**: Batched translation + optimized inference = 1-2 min for 1h video
- üì± **Web Interface**: Simple browser UI with real-time progress
- üîÑ **Hierarchical Sections**: Main topics + subsections automatically detected
- üé® **Clean Output**: Proper nouns preserved, no numeric garbage, grammatically correct

## üöÄ Quick Start

### Prerequisites
- Python 3.11+
- ~2GB disk space (for models)
- (Optional) DeepL API key for translation

### Installation

```bash
# Clone repository
git clone https://github.com/foersben/python-youtube-transcript.git
cd pythonyoutubetranscript

# Install dependencies
poetry install

# Manual CPU-only packages (prevent GPU bloat)
poetry run pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
poetry run pip install sentence-transformers chromadb

# Download local model (first run will auto-download if missing)
mkdir -p models
# Model will auto-download from HuggingFace on first use
```

### Configuration

Create `.env` file in project root:

```bash
# Optional: DeepL for translation (recommended)
DEEPL_API_KEY=your_deepl_key_here

# Local LLM Configuration
USE_LOCAL_LLM=true
LOCAL_MODEL_PATH=models/Phi-3-mini-4k-instruct-q4.gguf
USE_LLM_TITLES=true  # Multi-stage refinement

# Translation (auto-batching enabled)
USE_TRANSLATION=true

# RAG for long videos
USE_RAG=auto  # auto, always, or never
RAG_HIERARCHICAL=true
LLM_TEMPERATURE=0.05
```

### Run Web Interface

```bash
poetry run python src/web_app.py
# Open http://localhost:5000
```

## üìñ Usage

### Web Interface (Recommended)

1. **Start the server:**
   ```bash
   poetry run python src/web_app.py
   ```

2. **Open browser:**
   ```
   http://localhost:5000
   ```

3. **Generate sections:**
   - Paste YouTube URL (e.g., `https://www.youtube.com/watch?v=cZ9PHPta9v0`)
   - Adjust settings (optional)
   - Click "Generate Sections"
   - Wait 1-2 minutes for processing
   - Copy results or download as text file

### Example Output

**German Video Input** (1 hour):
```
https://www.youtube.com/watch?v=cZ9PHPta9v0
```

**Generated German Sections** (back-translated from English):
```
1. 12:02 Studentenrat Universit√§tspolitik
2.   16:03 Israelische Besatzung Debatte
3.   20:04 Mediale Wahrheitsanspr√ºche
4. 24:05 Deutsch-Israelischer Dialog
5.   28:06 Messkriterien Diskussion
6.   32:07 Begleitende Dokumentation
7. 36:08 Baumgarten Vorw√ºrfe
8.   40:09 Historische Genauigkeit
9.   44:11 Produktionsfirma Beteiligung
10. 48:12 Journalistische Integrit√§t
11.   52:13 Ukraine Geopolitik
12.   56:13 Zuk√ºnftige Bedenken
```

**Processing Time**: ~90 seconds
- Translation: ~30s (batched)
- RAG indexing: ~10s
- Section generation: ~40s
- Back-translation: ~10s

---

## üèóÔ∏è Architecture

### Multi-Stage LLM Pipeline

The system uses **3-stage refinement** to maximize quality from small models:

```
Stage 1: Extract Keywords
  Input:  200 chars of context
  Prompt: "Extract 3-5 key topics from this text:\n{snippet}\nTopics:"
  Output: "student council, university, politics"
  Time:   ~0.3s

Stage 2: Generate Title  
  Input:  Keywords from Stage 1
  Prompt: "Create a 2-4 word title from these topics: {keywords}\nUse nouns only.\nTitle:"
  Output: "Student council politics"
  Time:   ~0.5s

Stage 3: Polish (Code)
  Input:  Raw title from Stage 2
  Action: Remove artifacts, clean punctuation, capitalize
  Output: "Student council politics" (final)
  Time:   instant
```

**Why this works**: Breaking complex reasoning into simple steps allows small models (Phi-3-mini 4B) to excel where one-shot prompts fail.

### Translation Batching

**Problem**: Translating 1500+ segments individually = slow, loses context, hits rate limits

**Solution**: Batch segments into ~4500 char chunks with markers

```python
# Before: 1586 API calls for 1-hour video
for segment in transcript:
    translate(segment)  # ‚ùå 5-10 minutes, poor quality

# After: 32 batched API calls
batches = create_batches(transcript, max_size=4500)
for batch in batches:
    translate(batch_with_markers)  # ‚úÖ 30-60 seconds, maintains context
```

**Markers preserve timestamps**:
```
[SEG_0]First sentence here.
[SEG_1]Second sentence here.
‚Üí Translate entire batch ‚Üí Split by markers ‚Üí Reconstruct
```

**Result**: 50x fewer API calls, 10x faster, full context preserved

### RAG for Long Videos

Videos >30 minutes use Retrieval-Augmented Generation:

1. **Index**: Split transcript into ~1000 char chunks
2. **Embed**: sentence-transformers/all-MiniLM-L6-v2 (CPU)
3. **Store**: ChromaDB vector database
4. **Retrieve**: Semantic search for relevant context per section
5. **Generate**: LLM uses retrieved context + local text for titles

**Benefits**:
- Handles 5+ hour videos efficiently
- Better context awareness
- Hierarchical structure (main + subsections)

---

## ‚öôÔ∏è Configuration

### Environment Variables

```bash
# Translation (DeepL recommended, local fallback available)
DEEPL_API_KEY=your_key_here
USE_TRANSLATION=true

# Local LLM
USE_LOCAL_LLM=true
LOCAL_MODEL_PATH=models/Phi-3-mini-4k-instruct-q4.gguf
USE_LLM_TITLES=true  # Multi-stage refinement (default: true)

# RAG System
USE_RAG=auto  # auto (>30min videos), always, never
RAG_HIERARCHICAL=true  # Main sections + subsections
LLM_TEMPERATURE=0.05  # Low = focused, deterministic
```

### Performance Tuning

**Faster (heuristics only)**:
```bash
USE_LLM_TITLES=false
USE_TRANSLATION=false
```

**Better Quality (cloud translation)**:
```bash
DEEPL_API_KEY=your_key  # DeepL > local LLM translation
```

**Longer videos**:
```bash
USE_RAG=always
RAG_HIERARCHICAL=true
```

---

## üìä Performance Benchmarks

| Video Length | Translation | RAG Index | Generation | Total | API Calls |
|--------------|-------------|-----------|------------|-------|-----------|
| 10 min       | ~10s        | ~2s       | ~15s       | ~30s  | ~8        |
| 30 min       | ~20s        | ~5s       | ~25s       | ~50s  | ~15       |
| 60 min       | ~30s        | ~10s      | ~40s       | ~90s  | ~32       |
| 120 min      | ~60s        | ~20s      | ~80s       | ~3m   | ~60       |

*Tested on: Intel i5-8265U, 16GB RAM, CPU-only inference*

---

## üß™ Quality Metrics

### Title Quality (1-hour German video)

| Metric | Before | After Multi-Stage | Improvement |
|--------|--------|-------------------|-------------|
| **Usable titles** | 20% | 90%+ | **4.5x better** |
| **LLM success rate** | 10% | 60-70% | **6x better** |
| **No verb fragments** | 50% | 95%+ | **No "Look accompanying"** |
| **Proper German** | 0% | 100% | **Back-translation works** |

### Translation Performance

| Metric | Per-Segment | Batched | Improvement |
|--------|-------------|---------|-------------|
| **API calls (1h video)** | 1586 | 32 | **50x fewer** |
| **Time** | 5-10 min | 30-60s | **10x faster** |
| **Context preserved** | ‚ùå Lost | ‚úÖ Full | **Coherent paragraphs** |
| **Rate limiting** | Often | Never | **No retries** |

---

## üõ†Ô∏è Technology Stack

### Core Dependencies
- **youtube-transcript-api** - Transcript extraction
- **deepl** - Translation (optional, fallback to local LLM)
- **llama-cpp-python** - Local LLM inference (CPU-optimized)
- **langchain** + **langchain-community** - LLM framework
- **chromadb** - Vector database for RAG
- **sentence-transformers** - Embeddings (CPU)
- **torch** (CPU-only) - PyTorch for transformers
- **flask** - Web application
- **langdetect** - Language detection

### Models
- **Phi-3-mini-4k-instruct** (GGUF, 4-bit quantized) - ~1.3GB
- **all-MiniLM-L6-v2** (sentence-transformers) - ~90MB

**Total disk usage**: ~2GB (models + dependencies)

---

## üêõ Troubleshooting

### Translation Issues

**Problem**: Titles not translated back to German
```bash
# Check logs for:
INFO - üîÑ Back-translating 12 section titles from EN-US to DE
INFO - ‚úÖ Back-translated 12/12 titles to DE (0 failed)
```
**Solution**:
- Ensure `DEEPL_API_KEY` is set in `.env`
- Check DeepL API quota
- Verify `USE_TRANSLATION=true`
- Restart Flask server after `.env` changes

---

**Problem**: Translation too slow (5+ minutes)
```bash
# Old per-segment translation (broken):
INFO - Translating segment 1/1586
INFO - Translating segment 2/1586
... (1586 times)
```
**Solution**: Code is already fixed with batching. If you see this, you're running old code:
```bash
git pull  # Get latest batching implementation
poetry install
```
**Expected logs**:
```bash
INFO - Batched 1586 segments into 32 chunks
INFO - Translating batch 1/32 (120 segments)
INFO - ‚úÖ Translation complete: 1586 segments in 32 batches
```

---

### Title Quality Issues

**Problem**: Titles contain fragments like "Look accompanying", "Come fear"
```
‚ùå "Look accompanying"
‚ùå "This event not be"
‚ùå "Come fear"
```
**Solution**: Multi-stage pipeline should be enabled (default):
```bash
# In .env
USE_LLM_TITLES=true  # Should be true
```
**Check logs for**:
```bash
INFO - [llama.cpp] Refined main title at 723.0s: 'Student council' -> 'Student council politics'
```

**If LLM refinement is failing**, try disabling for faster heuristics-only:
```bash
USE_LLM_TITLES=false
```

---

**Problem**: Titles still in English instead of German

**Check**:
1. Is translation enabled? `USE_TRANSLATION=true`
2. Is source language detected? Look for: `INFO - Translating transcript from de to EN-US`
3. Is back-translation running? Look for: `INFO - üîÑ Back-translating ... to DE`

**Common cause**: Flask server needs restart after `.env` changes:
```bash
# Kill server (Ctrl+C)
poetry run python src/web_app.py  # Restart
```

---

### Model Loading Issues

**Problem**: Model file not found
```
ERROR - Model file not found: /home/.../models/Phi-3-mini-4k-instruct-q4.gguf
```
**Solution**:
1. Download model manually:
   ```bash
   mkdir -p models
   cd models
   wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
   ```

2. Or let it auto-download on first run (requires `huggingface_hub`):
   ```bash
   poetry run pip install huggingface-hub
   ```

---

**Problem**: GPU/CUDA errors despite CPU-only setup
```
ERROR - CUDA out of memory
ERROR - torchvision::nms does not exist
```
**Solution**: Clean GPU packages and reinstall CPU-only:
```bash
# Uninstall GPU versions
poetry run pip uninstall torch torchvision torchaudio -y

# Install CPU-only versions
poetry run pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
poetry run pip install sentence-transformers chromadb
```

---

### Performance Issues

**Problem**: Generation takes 5+ minutes for 1-hour video

**Expected times**:
- Translation: ~30-60s (batched)
- RAG indexing: ~10s
- Generation: ~40s
- Total: ~90-120s

**If slower**:
1. Check batching is working (see logs above)
2. Verify CPU has multiple cores (used for threading)
3. Disable LLM refinement for speed:
   ```bash
   USE_LLM_TITLES=false
   ```

---

**Problem**: High memory usage (>8GB)

**Solution**: Already optimized for CPU. If still high:
- Close other applications
- Use 4-bit quantized model (already default)
- Reduce batch size in code if needed

---

### Web Interface Issues

**Problem**: Port 5000 already in use
```bash
Address already in use
```
**Solution**:
```bash
# Find and kill process on port 5000
lsof -ti:5000 | xargs kill -9

# Or use different port
FLASK_RUN_PORT=5001 poetry run python src/web_app.py
```

---

**Problem**: "Module not found" errors
```bash
ModuleNotFoundError: No module named 'sentence_transformers'
```
**Solution**:
```bash
# Install manual packages (not managed by Poetry to avoid GPU bloat)
poetry run pip install sentence-transformers chromadb
poetry run pip install torch --index-url https://download.pytorch.org/whl/cpu
```

---

## üìö Documentation

### User Documentation
- **[Installation & Setup](README.md#installation)** - Get started quickly
- **[Web Interface Usage](README.md#web-interface-recommended)** - Browser-based processing
- **[CLI Usage](README.md#command-line)** - Command-line processing
- **[Configuration Guide](README.md#configuration)** - Environment variables and settings

### Technical Documentation
- **[Architecture & Design](docs/ARCHITECTURE.md)** - System design and implementation
- **[Processing Pipeline](README.md#processing-pipeline-detailed)** - Step-by-step breakdown
- **[Troubleshooting Guide](docs/TROUBLESHOOTING.md)** - Common issues and solutions
- **[Contributing Guide](docs/CONTRIBUTING.md)** - Development and contribution process

### Project History
- **[Changelog](docs/CHANGELOG.md)** - Chronological development history
- **[Documentation Index](docs/README.md)** - All documentation organized

---

## üéØ Project Status

‚úÖ **Production Ready**

- ‚úÖ Multi-stage LLM refinement working
- ‚úÖ Translation batching optimized (50x speedup)
- ‚úÖ RAG system for long videos
- ‚úÖ Back-translation to original language
- ‚úÖ CPU-only inference (no GPU required)
- ‚úÖ Web interface functional
- ‚úÖ Comprehensive documentation

**Known Limitations:**
- Requires ~2GB disk space (for models)
- CPU inference slower than GPU (but sufficient for production)
- Translation quality best with DeepL (local fallback available)
- Small model (Phi-3-mini 4B) occasionally produces weak titles (60-70% success rate, falls back to heuristics)

**Future Enhancements:**
- [ ] Optional GPU acceleration
- [ ] Cloud LLM integration (GPT-4, Claude) for better quality
- [ ] Translation caching per video
- [ ] Batch processing CLI
- [ ] Docker deployment

---

## ü§ù Contributing

We welcome contributions! Please see our **[Contributing Guide](docs/CONTRIBUTING.md)** for:
- Development setup instructions
- Code standards and style guidelines
- Testing requirements
- Pull request process

---

## üìß Contact

For issues, questions, or suggestions:
- **GitHub Issues**: [Report a bug](https://github.com/foersben/python-youtube-transcript/issues)
- **Documentation**: [Complete docs organized](docs/README.md)
- **Contributing**: [Development guide](docs/CONTRIBUTING.md)

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Microsoft Phi-3-mini**: Excellent small model for CPU inference
- **llama.cpp**: Fast CPU inference engine
- **DeepL API**: Professional translation service
- **ChromaDB**: Simple and effective vector database
- **Sentence Transformers**: Efficient embedding generation

---

<div align="center">

**Made with ‚ù§Ô∏è for the YouTube community**

[‚≠ê Star on GitHub](https://github.com/foersben/python-youtube-transcript) ‚Ä¢ [üìñ Documentation](docs/README.md) ‚Ä¢ [üêõ Report Bug](https://github.com/foersben/python-youtube-transcript/issues)

</div>
